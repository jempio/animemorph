{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APS360 Project: Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Data Loading\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(root_dir, fname) for fname in os.listdir(root_dir) if os.path.isfile(os.path.join(root_dir, fname))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, 0  # Since there are no labels, we can return a dummy label (0)\n",
    "\n",
    "def get_data_loader(data_dir, batch_size, image_size=(224, 224)):\n",
    "    # Define the transformation\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    # Verify the path\n",
    "    assert os.path.exists(data_dir), f\"Directory not found: {data_dir}\"\n",
    "\n",
    "    # Load the dataset\n",
    "    dataset = CustomImageDataset(root_dir=data_dir, transform=transform)\n",
    "\n",
    "    # Function to split dataset indices\n",
    "    def split_indices(dataset):\n",
    "        num_images = len(dataset)\n",
    "        indices = list(range(num_images))\n",
    "        np.random.seed(1000)  # Setting a seed for reproducibility\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        train_split = int(0.7 * num_images)\n",
    "        val_split = int(0.15 * num_images)\n",
    "\n",
    "        train_indices = indices[:train_split]\n",
    "        val_indices = indices[train_split:train_split + val_split]\n",
    "        test_indices = indices[train_split + val_split:]\n",
    "\n",
    "        return train_indices, val_indices, test_indices\n",
    "\n",
    "    # Get split indices for the dataset\n",
    "    train_indices, val_indices, test_indices = split_indices(dataset)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "    test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=1)\n",
    "    val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler, num_workers=1)\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler, num_workers=1)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def verify_splits_and_check_overlaps(train_loader_A, val_loader_A, test_loader_A, train_loader_B, val_loader_B, test_loader_B):\n",
    "    # Verify the split sizes for dataset A\n",
    "    total_examples_A = len(train_loader_A.sampler) + len(val_loader_A.sampler) + len(test_loader_A.sampler)\n",
    "    train_size_A = len(train_loader_A.sampler)\n",
    "    val_size_A = len(val_loader_A.sampler)\n",
    "    test_size_A = len(test_loader_A.sampler)\n",
    "\n",
    "    train_proportion_A = train_size_A / total_examples_A\n",
    "    val_proportion_A = val_size_A / total_examples_A \n",
    "    test_proportion_A = test_size_A / total_examples_A\n",
    "\n",
    "    print(f\"Dataset A - Total examples: {total_examples_A}\")\n",
    "    print(f\"Dataset A - Train examples: {train_size_A} ({train_proportion_A:.2%})\")\n",
    "    print(f\"Dataset A - Validation examples: {val_size_A} ({val_proportion_A:.2%})\")\n",
    "    print(f\"Dataset A - Test examples: {test_size_A} ({test_proportion_A:.2%})\")\n",
    "\n",
    "    # Verify the split sizes for dataset B\n",
    "    total_examples_B = len(train_loader_B.sampler) + len(val_loader_B.sampler) + len(test_loader_B.sampler)\n",
    "    train_size_B = len(train_loader_B.sampler)\n",
    "    val_size_B = len(val_loader_B.sampler)\n",
    "    test_size_B = len(test_loader_B.sampler)\n",
    "\n",
    "    train_proportion_B = train_size_B / total_examples_B\n",
    "    val_proportion_B = val_size_B / total_examples_B \n",
    "    test_proportion_B = test_size_B / total_examples_B\n",
    "\n",
    "    print(f\"Dataset B - Total examples: {total_examples_B}\")\n",
    "    print(f\"Dataset B - Train examples: {train_size_B} ({train_proportion_B:.2%})\")\n",
    "    print(f\"Dataset B - Validation examples: {val_size_B} ({val_proportion_B:.2%})\")\n",
    "    print(f\"Dataset B - Test examples: {test_size_B} ({test_proportion_B:.2%})\")\n",
    "\n",
    "    # Check for overlaps in dataset A\n",
    "    train_indices_A = list(train_loader_A.sampler.indices)\n",
    "    val_indices_A = list(val_loader_A.sampler.indices)\n",
    "    test_indices_A = list(test_loader_A.sampler.indices)\n",
    "    print(\"Checking overlaps for dataset A...\")\n",
    "    check_for_overlaps(train_indices_A, val_indices_A, test_indices_A)\n",
    "\n",
    "    # Check for overlaps in dataset B\n",
    "    train_indices_B = list(train_loader_B.sampler.indices)\n",
    "    val_indices_B = list(val_loader_B.sampler.indices)\n",
    "    test_indices_B = list(test_loader_B.sampler.indices)\n",
    "    print(\"Checking overlaps for dataset B...\")\n",
    "    check_for_overlaps(train_indices_B, val_indices_B, test_indices_B)\n",
    "\n",
    "def check_for_overlaps(train_indices, val_indices, test_indices):\n",
    "    # Convert indices to sets\n",
    "    train_indices_set = set(train_indices)\n",
    "    val_indices_set = set(val_indices)\n",
    "    test_indices_set = set(test_indices)\n",
    "\n",
    "    # Check for overlaps\n",
    "    train_val_overlap = train_indices_set.intersection(val_indices_set)\n",
    "    train_test_overlap = train_indices_set.intersection(test_indices_set)\n",
    "    val_test_overlap = val_indices_set.intersection(test_indices_set)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Train/Validation Overlap: {len(train_val_overlap)} indices\")\n",
    "    print(f\"Train/Test Overlap: {len(train_test_overlap)} indices\")\n",
    "    print(f\"Validation/Test Overlap: {len(val_test_overlap)} indices\")\n",
    "\n",
    "    if not train_val_overlap and not train_test_overlap and not val_test_overlap:\n",
    "        print(\"No overlaps found between train, validation, and test sets.\")\n",
    "    else:\n",
    "        print(\"Overlaps detected. Please check the data splitting logic.\")\n",
    "\n",
    "###############################################################################\n",
    "# Training\n",
    "def evaluate(net, loader, criterion):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    total_loss = 0.0\n",
    "    total_err = 0.0\n",
    "    total_samples = 0\n",
    "    net.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_err += (predicted != labels).sum().item()\n",
    "            total_loss += loss.item()\n",
    "            total_samples += labels.size(0)\n",
    "    err = total_err / total_samples\n",
    "    loss = total_loss / len(loader)\n",
    "    return err, loss\n",
    "\n",
    "def get_model_name(name, batch_size, learning_rate, epoch):\n",
    "    \n",
    "    path = \"model_{0}_bs{1}_lr{2}_epoch{3}\".format(name,\n",
    "                                                   batch_size,\n",
    "                                                   learning_rate,\n",
    "                                                   epoch)\n",
    "    return path\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Training Curve\n",
    "def plot_training_curve(path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    train_err = np.loadtxt(\"{}_train_err.csv\".format(path))\n",
    "    val_err = np.loadtxt(\"{}_val_err.csv\".format(path))\n",
    "    train_loss = np.loadtxt(\"{}_train_loss.csv\".format(path))\n",
    "    val_loss = np.loadtxt(\"{}_val_loss.csv\".format(path))\n",
    "    plt.title(\"Train vs Validation Error\")\n",
    "    n = len(train_err) # number of epochs\n",
    "    plt.plot(range(1,n+1), train_err, label=\"Train\")\n",
    "    plt.plot(range(1,n+1), val_err, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    plt.title(\"Train vs Validation Loss\")\n",
    "    plt.plot(range(1,n+1), train_loss, label=\"Train\")\n",
    "    plt.plot(range(1,n+1), val_loss, label=\"Validation\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading and Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for human faces\n",
    "data_dir_A = 'C:/Users/proga/Documents/GitHub/aps360-project/data/dataSetA_10k'\n",
    "batch_size = 32\n",
    "\n",
    "train_loader_A, val_loader_A, test_loader_A = get_data_loader(data_dir_A, batch_size, image_size=(128, 128))\n",
    "\n",
    "# Example usage for anime faces\n",
    "data_dir_B = 'C:/Users/proga/Documents/GitHub/aps360-project/data/dataSetB_10k'\n",
    "\n",
    "train_loader_B, val_loader_B, test_loader_B = get_data_loader(data_dir_B, batch_size, image_size=(128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Verify Split and Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset A - Total examples: 10000\n",
      "Dataset A - Train examples: 7000 (70.00%)\n",
      "Dataset A - Validation examples: 1500 (15.00%)\n",
      "Dataset A - Test examples: 1500 (15.00%)\n",
      "Dataset B - Total examples: 10000\n",
      "Dataset B - Train examples: 7000 (70.00%)\n",
      "Dataset B - Validation examples: 1500 (15.00%)\n",
      "Dataset B - Test examples: 1500 (15.00%)\n",
      "Checking overlaps for dataset A...\n",
      "Train/Validation Overlap: 0 indices\n",
      "Train/Test Overlap: 0 indices\n",
      "Validation/Test Overlap: 0 indices\n",
      "No overlaps found between train, validation, and test sets.\n",
      "Checking overlaps for dataset B...\n",
      "Train/Validation Overlap: 0 indices\n",
      "Train/Test Overlap: 0 indices\n",
      "Validation/Test Overlap: 0 indices\n",
      "No overlaps found between train, validation, and test sets.\n"
     ]
    }
   ],
   "source": [
    "# Check for overlaps\n",
    "verify_splits_and_check_overlaps(train_loader_A, val_loader_A, test_loader_A, train_loader_B, val_loader_B, test_loader_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Taken from Lab 2\n",
    "def visualize_data_loader(data_loader, title, num_images=1):\n",
    "    k = 0\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for images, labels in data_loader:\n",
    "        for i in range(images.size(0)):  \n",
    "            if k >= num_images:\n",
    "                break\n",
    "            image = images[i]\n",
    "            img = np.transpose(image.numpy(), [1, 2, 0])\n",
    "            img = img / 2 + 0.5\n",
    "            plt.subplot(3, 5, k+1)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(img)\n",
    "            k += 1\n",
    "        if k >= num_images:\n",
    "            break\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize dataset A\n",
    "visualize_data_loader(train_loader_A, title=\"Dataset A - Train Loader\")\n",
    "visualize_data_loader(val_loader_A, title=\"Dataset A - Validation Loader\")\n",
    "visualize_data_loader(test_loader_A, title=\"Dataset A - Test Loader\")\n",
    "\n",
    "# Visualize dataset B\n",
    "visualize_data_loader(train_loader_B, title=\"Dataset B - Train Loader\")\n",
    "visualize_data_loader(val_loader_B, title=\"Dataset B - Validation Loader\")\n",
    "visualize_data_loader(test_loader_B, title=\"Dataset B - Test Loader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Building and Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            self._block(z_dim, features_g*32, 4, 1, 0), # N x f_g*32 x 4 x 4\n",
    "            self._block(features_g*32, features_g*16, 4, 2, 1), #8 x 8\n",
    "            self._block(features_g*16, features_g*8, 4, 2, 1), #16 x 16\n",
    "            self._block(features_g*8, features_g*4, 4, 2, 1), #32 x 32\n",
    "            self._block(features_g*4, features_g*2, 4, 2, 1), #64 x 64\n",
    "            nn.ConvTranspose2d(features_g*2, channels_img, kernel_size=4, stride=2, padding=1,), # 128 x 128\n",
    "            nn.Tanh(), #[-1, 1]\n",
    "        )\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "\n",
    "        for i, layer in enumerate(self.gen):\n",
    "            x = layer(x)\n",
    "            print(f\"Layer {i} output size: {x.size()}\")\n",
    "        print(\"\\n\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "  def __init__(self, channels_img, features_d):\n",
    "    super(Discriminator, self).__init__()\n",
    "    self.disc = nn.Sequential(\n",
    "        #reducing the dimension of the image from 64x64 to simply 1\n",
    "        #after applying convolution once, apply batch norm multiple times\n",
    "        nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1), #32x32\n",
    "        nn.LeakyReLU(0.2),\n",
    "        self._block(features_d, features_d*2, 4, 2, 1), #16x16\n",
    "        self._block(features_d*2, features_d*4, 4, 2, 1), #8x8\n",
    "        self._block(features_d*4, features_d*8, 4, 2, 1), #4x4\n",
    "        nn.Conv2d(features_d*8, 1, kernel_size=4, stride=2, padding=0), #1x1\n",
    "        #returns 0 or 1 using sigmoid function (binary classification)\n",
    "        nn.Sigmoid(),\n",
    "    )\n",
    "  \n",
    "  def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False,),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.LeakyReLU(0.2),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.disc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weight(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "True\n",
      "1\n",
      "0\n",
      "NVIDIA GeForce GTX 1660 SUPER\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_default_device('cuda')\n",
    "print(torch.rand(10).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#most from DCGAN paper, some minor changes\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 1e-5\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 128\n",
    "CHANNELS_IMG = 1\n",
    "Z_DIM = 100\n",
    "NUM_EPOCHS = 50\n",
    "FEAUTERS_DISC = 64\n",
    "FEAUTRES_GEN = 64\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(1000)\n",
    "\n",
    "transforms = transforms.Compose( [\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)])\n",
    "])\n",
    "\n",
    "gen = Generator(Z_DIM,CHANNELS_IMG,FEAUTRES_GEN).to(device)\n",
    "disc = Discriminator(CHANNELS_IMG, FEAUTERS_DISC).to(device)\n",
    "initialize_weight(gen)\n",
    "initialize_weight(disc)\n",
    "\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(32, Z_DIM, 1, 1).to(device)\n",
    "\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/real\")\n",
    "\n",
    "step = 0\n",
    "\n",
    "gen.train()\n",
    "disc.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch_idx, (real, _) in enumerate(train_loader_A):\n",
    "        real = real.to(device)\n",
    "        noise = torch.randn(BATCH_SIZE, Z_DIM, 1, 1).to(device)\n",
    "        fake = gen(noise)\n",
    "        disc_real = disc(real).reshape(-1)\n",
    "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake.detach()).reshape(-1)\n",
    "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "        disc.zero_grad()\n",
    "        loss_disc.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "        output = disc(fake).reshape(-1)\n",
    "        loss_gen = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(train_loader_A)} \\\n",
    "                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise)\n",
    "                img_grid_real = torchvision.utils.make_grid(real[:32], normalize=True)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
    "\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "            step += 1\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}] Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\")\n",
    "print(\"Training Done\")\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
