{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# APS360 Project: Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part A. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import shutil\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "mp.set_start_method('spawn', force=True)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TRAIN_DIR = \"data/train\"\n",
    "VAL_DIR = \"data/val\"\n",
    "BATCH_SIZE = 32\n",
    "LEARNING_RATE = 0.0002\n",
    "LAMBDA_IDENTITY = 0.0  # loss weight for identity loss\n",
    "LAMBDA_CYCLE = 10\n",
    "NUM_WORKERS = 4\n",
    "NUM_EPOCHS = 50\n",
    "LOAD_MODEL = True\n",
    "SAVE_MODEL = True\n",
    "CHECKPOINT_GENERATOR_ANIME = \"models/gen_anime\"\n",
    "CHECKPOINT_GENERATOR_HUMAN = \"models/gen_human\"\n",
    "CHECKPOINT_DISCRIMINATOR_ANIME = \"models/disc_anime\"\n",
    "CHECKPOINT_DISCRIMINATOR_HUMAN = \"models/disc_human\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Data Loading\n",
    "import os\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(root_dir, fname) for fname in os.listdir(root_dir) if os.path.isfile(os.path.join(root_dir, fname))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, 0  # Since there are no labels, we can return a dummy label (0)\n",
    "\n",
    "def create_subset_dataset(original_dir, subset_dir, subset_size=100):\n",
    "    if not os.path.exists(subset_dir):\n",
    "        os.makedirs(subset_dir)\n",
    "    \n",
    "    image_paths = [os.path.join(original_dir, fname) for fname in os.listdir(original_dir) if os.path.isfile(os.path.join(original_dir, fname))]\n",
    "    \n",
    "    np.random.seed(1000)\n",
    "    np.random.shuffle(image_paths)\n",
    "    subset_paths = image_paths[:subset_size]\n",
    "    \n",
    "    for img_path in subset_paths:\n",
    "        shutil.copy(img_path, subset_dir)\n",
    "\n",
    "    print(f\"Subset created with {len(subset_paths)} images in {subset_dir}\")\n",
    "\n",
    "def get_data_loader(data_dir, batch_size, image_size=(224, 224), subset_size=None):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize(image_size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "    \n",
    "    assert os.path.exists(data_dir), f\"Directory not found: {data_dir}\"\n",
    "\n",
    "    dataset = CustomImageDataset(root_dir=data_dir, transform=transform)\n",
    "\n",
    "    if subset_size:\n",
    "        num_images = len(dataset)\n",
    "        indices = list(range(num_images))\n",
    "        np.random.seed(1000)\n",
    "        np.random.shuffle(indices)\n",
    "        subset_indices = indices[:subset_size]\n",
    "        subset_sampler = SubsetRandomSampler(subset_indices)\n",
    "        loader = DataLoader(dataset, batch_size=batch_size, sampler=subset_sampler, num_workers=1)\n",
    "        return loader\n",
    "\n",
    "    def split_indices(dataset):\n",
    "        num_images = len(dataset)\n",
    "        indices = list(range(num_images))\n",
    "        np.random.seed(1000)\n",
    "        np.random.shuffle(indices)\n",
    "\n",
    "        train_split = int(0.7 * num_images)\n",
    "        val_split = int(0.15 * num_images)\n",
    "\n",
    "        train_indices = indices[:train_split]\n",
    "        val_indices = indices[train_split:train_split + val_split]\n",
    "        test_indices = indices[train_split + val_split:]\n",
    "\n",
    "        return train_indices, val_indices, test_indices\n",
    "\n",
    "    train_indices, val_indices, test_indices = split_indices(dataset)\n",
    "\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    val_sampler = SubsetRandomSampler(val_indices)\n",
    "    test_sampler = SubsetRandomSampler(test_indices)\n",
    "\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=1)\n",
    "    val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler, num_workers=1)\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler, num_workers=1)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "def verify_splits_and_check_overlaps(train_loader_A, val_loader_A, test_loader_A, train_loader_B, val_loader_B, test_loader_B):\n",
    "    # Verify the split sizes for dataset A\n",
    "    total_examples_A = len(train_loader_A.sampler) + len(val_loader_A.sampler) + len(test_loader_A.sampler)\n",
    "    train_size_A = len(train_loader_A.sampler)\n",
    "    val_size_A = len(val_loader_A.sampler)\n",
    "    test_size_A = len(test_loader_A.sampler)\n",
    "\n",
    "    train_proportion_A = train_size_A / total_examples_A\n",
    "    val_proportion_A = val_size_A / total_examples_A \n",
    "    test_proportion_A = test_size_A / total_examples_A\n",
    "\n",
    "    print(f\"Dataset A - Total examples: {total_examples_A}\")\n",
    "    print(f\"Dataset A - Train examples: {train_size_A} ({train_proportion_A:.2%})\")\n",
    "    print(f\"Dataset A - Validation examples: {val_size_A} ({val_proportion_A:.2%})\")\n",
    "    print(f\"Dataset A - Test examples: {test_size_A} ({test_proportion_A:.2%})\")\n",
    "\n",
    "    # Verify the split sizes for dataset B\n",
    "    total_examples_B = len(train_loader_B.sampler) + len(val_loader_B.sampler) + len(test_loader_B.sampler)\n",
    "    train_size_B = len(train_loader_B.sampler)\n",
    "    val_size_B = len(val_loader_B.sampler)\n",
    "    test_size_B = len(test_loader_B.sampler)\n",
    "\n",
    "    train_proportion_B = train_size_B / total_examples_B\n",
    "    val_proportion_B = val_size_B / total_examples_B \n",
    "    test_proportion_B = test_size_B / total_examples_B\n",
    "\n",
    "    print(f\"Dataset B - Total examples: {total_examples_B}\")\n",
    "    print(f\"Dataset B - Train examples: {train_size_B} ({train_proportion_B:.2%})\")\n",
    "    print(f\"Dataset B - Validation examples: {val_size_B} ({val_proportion_B:.2%})\")\n",
    "    print(f\"Dataset B - Test examples: {test_size_B} ({test_proportion_B:.2%})\")\n",
    "\n",
    "    # Check for overlaps in dataset A\n",
    "    train_indices_A = list(train_loader_A.sampler.indices)\n",
    "    val_indices_A = list(val_loader_A.sampler.indices)\n",
    "    test_indices_A = list(test_loader_A.sampler.indices)\n",
    "    print(\"Checking overlaps for dataset A...\")\n",
    "    check_for_overlaps(train_indices_A, val_indices_A, test_indices_A)\n",
    "\n",
    "    # Check for overlaps in dataset B\n",
    "    train_indices_B = list(train_loader_B.sampler.indices)\n",
    "    val_indices_B = list(val_loader_B.sampler.indices)\n",
    "    test_indices_B = list(test_loader_B.sampler.indices)\n",
    "    print(\"Checking overlaps for dataset B...\")\n",
    "    check_for_overlaps(train_indices_B, val_indices_B, test_indices_B)\n",
    "\n",
    "def check_for_overlaps(train_indices, val_indices, test_indices):\n",
    "    # Convert indices to sets\n",
    "    train_indices_set = set(train_indices)\n",
    "    val_indices_set = set(val_indices)\n",
    "    test_indices_set = set(test_indices)\n",
    "\n",
    "    # Check for overlaps\n",
    "    train_val_overlap = train_indices_set.intersection(val_indices_set)\n",
    "    train_test_overlap = train_indices_set.intersection(test_indices_set)\n",
    "    val_test_overlap = val_indices_set.intersection(test_indices_set)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Train/Validation Overlap: {len(train_val_overlap)} indices\")\n",
    "    print(f\"Train/Test Overlap: {len(train_test_overlap)} indices\")\n",
    "    print(f\"Validation/Test Overlap: {len(val_test_overlap)} indices\")\n",
    "\n",
    "    if not train_val_overlap and not train_test_overlap and not val_test_overlap:\n",
    "        print(\"No overlaps found between train, validation, and test sets.\")\n",
    "    else:\n",
    "        print(\"Overlaps detected. Please check the data splitting logic.\")\n",
    "\n",
    "###############################################################################\n",
    "# Training\n",
    "import glob\n",
    "\n",
    "def evaluate(generator, discriminator, loader, criterion, device):\n",
    "    generator.eval()\n",
    "    discriminator.eval()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            \n",
    "            # Generate fake images\n",
    "            fake_images = generator(inputs)\n",
    "            outputs = discriminator(fake_images)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, torch.ones_like(outputs).to(device))\n",
    "            total_loss += loss.item()\n",
    "            total_samples += inputs.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    return avg_loss\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    print(f\"=> Saving checkpoint to {filename}\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "def load_latest_checkpoint(model, optimizer, checkpoint_prefix, lr, batch_size):\n",
    "    checkpoint_files = glob.glob(f\"{checkpoint_prefix}_lr{lr}_bs{batch_size}_epoch_*.pth.tar\")\n",
    "    if not checkpoint_files:\n",
    "        print(f\"=> No checkpoint found for {checkpoint_prefix}. Starting from scratch.\")\n",
    "        return 0\n",
    "\n",
    "    # Extract the latest epoch number from the checkpoint file names\n",
    "    latest_epoch = max([int(file.split('_')[-1].split('.')[0]) for file in checkpoint_files])\n",
    "    checkpoint_file = f\"{checkpoint_prefix}_lr{lr}_bs{batch_size}_epoch_{latest_epoch}.pth.tar\"\n",
    "\n",
    "    print(f\"=> Loading checkpoint {checkpoint_file}\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "    return latest_epoch + 1\n",
    "\n",
    "def get_model_name(model_type, epoch, learning_rate, batch_size):\n",
    "    return f\"models/{model_type}_lr{learning_rate}_bs{batch_size}_epoch_{epoch}.pth.tar\"\n",
    "\n",
    "def list_checkpoints(checkpoint_prefix):\n",
    "    checkpoint_pattern = os.path.join(f\"{checkpoint_prefix}_epoch_*.pth.tar\")\n",
    "    checkpoint_files = glob.glob(checkpoint_pattern)\n",
    "    \n",
    "    if not checkpoint_files:\n",
    "        print(f\"No checkpoint found for prefix: {checkpoint_prefix}\")\n",
    "    else:\n",
    "        print(f\"Checkpoints found for prefix {checkpoint_prefix}:\")\n",
    "        for checkpoint in checkpoint_files:\n",
    "            print(checkpoint)\n",
    "            \n",
    "def save_losses(train_loss_anime_history, train_loss_human_history, val_loss_anime_history, val_loss_human_history, path_prefix):\n",
    "    np.savetxt(f\"{path_prefix}_train_loss_anime.csv\", train_loss_anime_history)\n",
    "    np.savetxt(f\"{path_prefix}_train_loss_human.csv\", train_loss_human_history)\n",
    "    np.savetxt(f\"{path_prefix}_val_loss_anime.csv\", val_loss_anime_history)\n",
    "    np.savetxt(f\"{path_prefix}_val_loss_human.csv\", val_loss_human_history)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "# Training Curve\n",
    "def plot_training_curve(path):\n",
    "    import matplotlib.pyplot as plt\n",
    "    train_loss_anime_file = f\"{path}_train_loss_anime.csv\"\n",
    "    train_loss_human_file = f\"{path}_train_loss_human.csv\"\n",
    "    val_loss_anime_file = f\"{path}_val_loss_anime.csv\"\n",
    "    val_loss_human_file = f\"{path}_val_loss_human.csv\"\n",
    "\n",
    "    train_loss_anime = np.loadtxt(train_loss_anime_file)\n",
    "    train_loss_human = np.loadtxt(train_loss_human_file)\n",
    "    val_loss_anime = np.loadtxt(val_loss_anime_file)\n",
    "    val_loss_human = np.loadtxt(val_loss_human_file)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Train vs Validation Loss (Anime)\")\n",
    "    n = len(train_loss_anime)  # number of epochs\n",
    "    plt.plot(range(1, n + 1), train_loss_anime, label=\"Train Anime\")\n",
    "    plt.plot(range(1, n + 1), val_loss_anime, label=\"Validation Anime\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"Train vs Validation Loss (Human)\")\n",
    "    plt.plot(range(1, n + 1), train_loss_human, label=\"Train Human\")\n",
    "    plt.plot(range(1, n + 1), val_loss_human, label=\"Validation Human\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Loading and Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset created with 50 images in data/subset_dataSetA\n",
      "Subset created with 50 images in data/subset_dataSetB\n"
     ]
    }
   ],
   "source": [
    "# Use smaller subsets for quick testing\n",
    "create_subset_dataset('C:/Users/proga/Documents/GitHub/aps360-project/data/dataSetA_10k', 'data/subset_dataSetA', subset_size=50)\n",
    "create_subset_dataset('C:/Users/proga/Documents/GitHub/aps360-project/data/dataSetB_10k', 'data/subset_dataSetB', subset_size=50)\n",
    "\n",
    "# Load data from the subsets\n",
    "train_loader_A, val_loader_A, test_loader_A = get_data_loader('data/subset_dataSetA', BATCH_SIZE, image_size=(128, 128))\n",
    "train_loader_B, val_loader_B, test_loader_B = get_data_loader('data/subset_dataSetB', BATCH_SIZE, image_size=(128, 128))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Verify Split and Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset A - Total examples: 50\n",
      "Dataset A - Train examples: 35 (70.00%)\n",
      "Dataset A - Validation examples: 7 (14.00%)\n",
      "Dataset A - Test examples: 8 (16.00%)\n",
      "Dataset B - Total examples: 50\n",
      "Dataset B - Train examples: 35 (70.00%)\n",
      "Dataset B - Validation examples: 7 (14.00%)\n",
      "Dataset B - Test examples: 8 (16.00%)\n",
      "Checking overlaps for dataset A...\n",
      "Train/Validation Overlap: 0 indices\n",
      "Train/Test Overlap: 0 indices\n",
      "Validation/Test Overlap: 0 indices\n",
      "No overlaps found between train, validation, and test sets.\n",
      "Checking overlaps for dataset B...\n",
      "Train/Validation Overlap: 0 indices\n",
      "Train/Test Overlap: 0 indices\n",
      "Validation/Test Overlap: 0 indices\n",
      "No overlaps found between train, validation, and test sets.\n"
     ]
    }
   ],
   "source": [
    "# Check for overlaps\n",
    "verify_splits_and_check_overlaps(train_loader_A, val_loader_A, test_loader_A, train_loader_B, val_loader_B, test_loader_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 7328) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\proga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1132\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1133\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1134\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\proga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m\n\u001b[0;32m     21\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Visualize dataset A\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[43mvisualize_data_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader_A\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDataset A - Train Loader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m visualize_data_loader(val_loader_A, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset A - Validation Loader\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     26\u001b[0m visualize_data_loader(test_loader_A, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset A - Test Loader\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m, in \u001b[0;36mvisualize_data_loader\u001b[1;34m(data_loader, title, num_images)\u001b[0m\n\u001b[0;32m      5\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m----> 7\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43m  \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnum_images\u001b[49m\u001b[43m:\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\proga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\proga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1329\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[0;32m   1328\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m-> 1329\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1330\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\proga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1295\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1292\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m-> 1295\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1296\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[0;32m   1297\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\proga\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1145\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1146\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[0;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 7328) exited unexpectedly"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Taken from Lab 2\n",
    "def visualize_data_loader(data_loader, title, num_images=3):\n",
    "    k = 0\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for images, labels in data_loader:\n",
    "        for i in range(images.size(0)):  \n",
    "            if k >= num_images:\n",
    "                break\n",
    "            image = images[i]\n",
    "            img = np.transpose(image.numpy(), [1, 2, 0])\n",
    "            img = img / 2 + 0.5\n",
    "            plt.subplot(3, 5, k+1)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(img)\n",
    "            k += 1\n",
    "        if k >= num_images:\n",
    "            break\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize dataset A\n",
    "visualize_data_loader(train_loader_A, title=\"Dataset A - Train Loader\")\n",
    "visualize_data_loader(val_loader_A, title=\"Dataset A - Validation Loader\")\n",
    "visualize_data_loader(test_loader_A, title=\"Dataset A - Test Loader\")\n",
    "\n",
    "# Visualize dataset B\n",
    "visualize_data_loader(train_loader_B, title=\"Dataset B - Train Loader\")\n",
    "visualize_data_loader(val_loader_B, title=\"Dataset B - Validation Loader\")\n",
    "visualize_data_loader(test_loader_B, title=\"Dataset B - Test Loader\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Building and Sanity Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, channels_img, features_g):\n",
    "        super(Generator, self).__init__()\n",
    "        self.gen = nn.Sequential(\n",
    "            self._block(z_dim, features_g*32, 4, 1, 0), # N x f_g*32 x 4 x 4\n",
    "            self._block(features_g*32, features_g*16, 4, 2, 1), #8 x 8\n",
    "            self._block(features_g*16, features_g*8, 4, 2, 1), #16 x 16\n",
    "            self._block(features_g*8, features_g*4, 4, 2, 1), #32 x 32\n",
    "            self._block(features_g*4, features_g*2, 4, 2, 1), #64 x 64\n",
    "            nn.ConvTranspose2d(features_g*2, channels_img, kernel_size=4, stride=2, padding=1,), # 128 x 128\n",
    "            nn.Tanh(), #[-1, 1]\n",
    "        )\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size,\n",
    "                stride,\n",
    "                padding,\n",
    "                bias=False\n",
    "            ),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "\n",
    "        for i, layer in enumerate(self.gen):\n",
    "            x = layer(x)\n",
    "            print(f\"Layer {i} output size: {x.size()}\")\n",
    "        print(\"\\n\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "  def __init__(self, channels_img, features_d):\n",
    "    super(Discriminator, self).__init__()\n",
    "    self.disc = nn.Sequential(\n",
    "        #reducing the dimension of the image from 64x64 to simply 1\n",
    "        #after applying convolution once, apply batch norm multiple times\n",
    "        nn.Conv2d(channels_img, features_d, kernel_size=4, stride=2, padding=1), #32x32\n",
    "        nn.LeakyReLU(0.2),\n",
    "        self._block(features_d, features_d*2, 4, 2, 1), #16x16\n",
    "        self._block(features_d*2, features_d*4, 4, 2, 1), #8x8\n",
    "        self._block(features_d*4, features_d*8, 4, 2, 1), #4x4\n",
    "        nn.Conv2d(features_d*8, 1, kernel_size=4, stride=2, padding=0), #1x1\n",
    "        #returns 0 or 1 using sigmoid function (binary classification)\n",
    "        nn.Sigmoid(),\n",
    "    )\n",
    "  \n",
    "  def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False,),\n",
    "        nn.BatchNorm2d(out_channels),\n",
    "        nn.LeakyReLU(0.2),\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    return self.disc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Weight Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weight(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Training Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "True\n",
      "1\n",
      "0\n",
      "NVIDIA GeForce GTX 1660 SUPER\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.device_count())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#torch.set_default_tensor_type(torch.cuda.FloatTensor)\n",
    "torch.set_default_dtype(torch.float32)\n",
    "torch.set_default_device('cuda')\n",
    "print(torch.rand(10).device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#most from DCGAN paper, some minor changes\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LEARNING_RATE = 1e-5\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = 128\n",
    "CHANNELS_IMG = 1\n",
    "Z_DIM = 100\n",
    "NUM_EPOCHS = 50\n",
    "FEAUTERS_DISC = 64\n",
    "FEAUTRES_GEN = 64\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "torch.manual_seed(1000)\n",
    "\n",
    "transforms = transforms.Compose( [\n",
    "    transforms.Resize(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5 for _ in range(CHANNELS_IMG)], [0.5 for _ in range(CHANNELS_IMG)])\n",
    "])\n",
    "\n",
    "gen = Generator(Z_DIM,CHANNELS_IMG,FEAUTRES_GEN).to(device)\n",
    "disc = Discriminator(CHANNELS_IMG, FEAUTERS_DISC).to(device)\n",
    "initialize_weight(gen)\n",
    "initialize_weight(disc)\n",
    "\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "opt_disc = optim.Adam(disc.parameters(), lr=LEARNING_RATE, betas=(0.5, 0.999))\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "fixed_noise = torch.randn(32, Z_DIM, 1, 1).to(device)\n",
    "\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "writer_fake = SummaryWriter(f\"logs/real\")\n",
    "\n",
    "step = 0\n",
    "\n",
    "gen.train()\n",
    "disc.train()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch_idx, (real, _) in enumerate(train_loader_A):\n",
    "        real = real.to(device)\n",
    "        noise = torch.randn(BATCH_SIZE, Z_DIM, 1, 1).to(device)\n",
    "        fake = gen(noise)\n",
    "        disc_real = disc(real).reshape(-1)\n",
    "        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n",
    "        disc_fake = disc(fake.detach()).reshape(-1)\n",
    "        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n",
    "        loss_disc = (loss_disc_real + loss_disc_fake) / 2\n",
    "        disc.zero_grad()\n",
    "        loss_disc.backward(retain_graph=True)\n",
    "        opt_disc.step()\n",
    "\n",
    "        output = disc(fake).reshape(-1)\n",
    "        loss_gen = criterion(output, torch.ones_like(output))\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f\"Epoch [{epoch}/{NUM_EPOCHS}] Batch {batch_idx}/{len(train_loader_A)} \\\n",
    "                  Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\")\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(fixed_noise)\n",
    "                img_grid_real = torchvision.utils.make_grid(real[:32], normalize=True)\n",
    "                img_grid_fake = torchvision.utils.make_grid(fake[:32], normalize=True)\n",
    "\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                writer_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "            step += 1\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}] Loss D: {loss_disc:.4f}, loss G: {loss_gen:.4f}\")\n",
    "print(\"Training Done\")\n",
    "end_time = time.time()\n",
    "print(f\"Time taken: {end_time - start_time}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
